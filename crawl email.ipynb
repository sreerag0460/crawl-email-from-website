{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_id='replace with your sheets id'\n",
    "sheet_name='Replace with the sheet name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import gspread\n",
    "from gspread_dataframe import set_with_dataframe\n",
    "from google.oauth2.service_account import Credentials\n",
    "\n",
    "# 1. Load your service account credentials\n",
    "SERVICE_ACCOUNT_FILE = r'Path to the service account'\n",
    "\n",
    "SCOPES = [\n",
    "    \"https://www.googleapis.com/auth/spreadsheets\",\n",
    "    \"https://www.googleapis.com/auth/drive\"\n",
    "]\n",
    "credentials = Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "\n",
    "# 2. Connect to Google Sheets\n",
    "client = gspread.authorize(credentials)\n",
    "\n",
    "# Replace with your Google Sheet ID\n",
    "\n",
    "sheet = client.open_by_key(sheet_id).worksheet(sheet_name)\n",
    "\n",
    "# 3. Read the existing data from the sheet\n",
    "data = sheet.get_all_values() \n",
    "\n",
    "data=pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.rename(columns={0: 'cmpny',1:'website'})\n",
    "data['email']=None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from queue import Queue, Empty\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#data=pd.read_csv(r'C:\\Users\\admin\\OneDrive\\Desktop\\work\\test.csv')\n",
    "\n",
    "\n",
    "class EmailScraper:\n",
    "    def __init__(self, base_url, max_workers=10, queue_size=100):\n",
    "        self.base_url = base_url\n",
    "        self.root_url = '{}://{}'.format(urlparse(self.base_url).scheme, urlparse(self.base_url).netloc)\n",
    "        self.pool = ThreadPoolExecutor(max_workers=max_workers)\n",
    "        self.scraped_pages = set()\n",
    "        self.to_crawl = Queue(queue_size)\n",
    "        self.to_crawl.put(self.base_url)\n",
    "        self.emails = set()  # Store unique emails\n",
    "\n",
    "    def parse_links(self, html):\n",
    "        \"\"\"Extracts all valid links from the given HTML page and adds them to the crawl queue.\"\"\"\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        links = soup.find_all('a', href=True)\n",
    "        for link in links:\n",
    "            url = urljoin(self.root_url, link['href'])\n",
    "            if url.startswith(self.root_url) and url not in self.scraped_pages:\n",
    "                self.scraped_pages.add(url)\n",
    "                try:\n",
    "                    self.to_crawl.put(url, timeout=1)\n",
    "                except:\n",
    "                    pass  # Ignore queue full errors\n",
    "\n",
    "    def extract_emails(self, html):\n",
    "        \"\"\"Extracts email addresses from the page content.\"\"\"\n",
    "        global data\n",
    "        emails_found = set(re.findall(r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\", html))\n",
    "        new_emails = emails_found - self.emails  # Avoid duplicates\n",
    "\n",
    "        if new_emails:\n",
    "            self.emails.update(new_emails)\n",
    "            print(f\"[{self.base_url}] Emails found: {new_emails}\")\n",
    "\n",
    "            # Ensure the website is in the DataFrame\n",
    "            if self.base_url not in data['website'].values:\n",
    "                data.loc[len(data)] = [self.base_url, str(new_emails)]\n",
    "            else:\n",
    "                # Fix: Convert NaN to an empty set before updating\n",
    "                existing_emails = data.loc[data['website'] == self.base_url, 'email'].values[0]\n",
    "                \n",
    "                if pd.isna(existing_emails):  # If NaN, initialize as an empty set\n",
    "                    existing_emails = set()\n",
    "                else:\n",
    "                    existing_emails = eval(existing_emails) if isinstance(existing_emails, str) else existing_emails\n",
    "\n",
    "                existing_emails.update(new_emails)  # Update with new emails\n",
    "                data.loc[data['website'] == self.base_url, 'email'] = str(existing_emails)\n",
    "\n",
    "    def post_scrape_callback(self, res):\n",
    "        \"\"\"Processes a webpage once it's downloaded.\"\"\"\n",
    "        try:\n",
    "            result = res.result()\n",
    "            if result and result.status_code == 200:\n",
    "                self.parse_links(result.text)  # Extract and queue links\n",
    "                self.extract_emails(result.text)  # Extract emails\n",
    "        except Exception as e:\n",
    "            print(f\"Error in callback: {e}\")\n",
    "\n",
    "    def scrape_page(self, url):\n",
    "        \"\"\"Fetches a webpage and returns the response.\"\"\"\n",
    "        try:\n",
    "            headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "            res = requests.get(url, headers=headers, timeout=(3, 30))  # Fetch page\n",
    "            time.sleep(1)  # Be polite to the server\n",
    "            return res\n",
    "        except requests.RequestException:\n",
    "            return None  # Handle network errors gracefully\n",
    "\n",
    "    def run_scraper(self):\n",
    "        \"\"\"Main loop: fetches URLs from the queue and processes them.\"\"\"\n",
    "        while True:\n",
    "            try:\n",
    "                target_url = self.to_crawl.get(timeout=60)  # Get next URL\n",
    "                if target_url not in self.scraped_pages:\n",
    "                    print(f\"Scraping URL: {target_url}\")\n",
    "                    self.scraped_pages.add(target_url)\n",
    "                    job = self.pool.submit(self.scrape_page, target_url)\n",
    "                    job.add_done_callback(self.post_scrape_callback)\n",
    "            except Empty:\n",
    "                print(f\"[{self.base_url}] Queue is empty. Exiting...\")\n",
    "                return\n",
    "            except Exception as e:\n",
    "                print(f\"Unexpected error: {e}\")\n",
    "                continue\n",
    "\n",
    "def run_multiple_scrapers(urls, max_threads=5):\n",
    "    \"\"\"Runs multiple scrapers concurrently for different websites.\"\"\"\n",
    "    with ThreadPoolExecutor(max_threads) as executor:\n",
    "        futures = {executor.submit(EmailScraper(url).run_scraper): url for url in urls}\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            try:\n",
    "                future.result()  # Ensure all scrapers complete\n",
    "            except Exception as e:\n",
    "                print(f\"Error in scraper for {futures[future]}: {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    urls = data['website']\n",
    "    run_multiple_scrapers(urls, max_threads=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gspread\n",
    "from gspread_dataframe import set_with_dataframe\n",
    "from google.oauth2.service_account import Credentials\n",
    "\n",
    "# 1. Load your service account credentials\n",
    "SERVICE_ACCOUNT_FILE = r'c:\\Users\\admin\\Downloads\\sreeragapac-2dee079eeff4.json'\n",
    "\n",
    "SCOPES = [\n",
    "    \"https://www.googleapis.com/auth/spreadsheets\",\n",
    "    \"https://www.googleapis.com/auth/drive\"\n",
    "]\n",
    "credentials = Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "\n",
    "# 2. Connect to Google Sheets\n",
    "client = gspread.authorize(credentials)\n",
    "\n",
    "# Replace with your Google Sheet ID\n",
    "\n",
    "sheet = client.open_by_key(sheet_id).worksheet(sheet_name)\n",
    "\n",
    "# 3. Read the existing data from the sheet\n",
    "existing_data = sheet.get_all_values()  # Get all values as a list of lists\n",
    "existing_row_count = len(existing_data)  # Count the number of existing rows (including the header)\n",
    "\n",
    "# 4. Load the new data from the CSV file\n",
    "new_data = data\n",
    "data=[data]\n",
    "\n",
    "# 5. Specify the starting row and column for the new data\n",
    "start_row = existing_row_count + 1  # Start after the last populated row\n",
    "start_col = 1  # Start in the first column (A)\n",
    "\n",
    "# 6. Write the new DataFrame to the Google Sheet starting at the specified row\n",
    "set_with_dataframe(sheet, new_data, row=start_row, col=start_col)\n",
    "\n",
    "print(f\"New data appended successfully starting at row {start_row}!\")\n",
    "final_data = sheet.get_all_values()\n",
    "print(final_data)#\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
